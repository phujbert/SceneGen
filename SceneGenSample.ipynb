{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5pfg9ElxSQwx",
        "QbsUzvacagJL",
        "y-b4grVJakDt",
        "B8rJflWNdqHx",
        "M8ta03hIbIhj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phujbert/SceneGen/blob/master/SceneGenSample.ipynb)"
      ],
      "metadata": {
        "id": "cUs_Fcb_hMT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import torchvision.utils as tutils\n",
        "from collections import OrderedDict\n",
        "import json"
      ],
      "metadata": {
        "id": "8tcVuAx6aDsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download trained models, graph dictionary, example graph"
      ],
      "metadata": {
        "id": "5pfg9ElxSQwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1C8OB3787DW1V-TNH8mvwGFvumcCAjr4b' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1C8OB3787DW1V-TNH8mvwGFvumcCAjr4b\" -O trained_models.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip trained_models.zip"
      ],
      "metadata": {
        "id": "x2Rq4memG2Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1PQZd63V8gaMQewPyPtTXWO1qMMAzOIHK' -O graph_dict.json"
      ],
      "metadata": {
        "id": "ewI1odCGKJif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Z2vZIViVdGebV_WDmrNNg0KuORzsH3vM' -O sample_graph.json"
      ],
      "metadata": {
        "id": "Hkxv3r1MLPe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion"
      ],
      "metadata": {
        "id": "QbsUzvacagJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Diffusion:\n",
        "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=64, device=\"cuda\"):\n",
        "        self.noise_steps = noise_steps\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "\n",
        "        self.beta = self.prepare_noise_schedule().to(device)\n",
        "        self.alpha = 1. - self.beta\n",
        "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.device = device\n",
        "\n",
        "    def prepare_noise_schedule(self):\n",
        "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
        "\n",
        "    def noise_images(self, x, t):\n",
        "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
        "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
        "        epsilon = torch.randn_like(x)\n",
        "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * epsilon, epsilon\n",
        "\n",
        "    def sample_timesteps(self, n):\n",
        "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
        "\n",
        "    def sample(self, model, n, layout, edge_dict=None):\n",
        "        print(\"start sample\")\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
        "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
        "                t = (torch.ones(n) * i).long().to(self.device)\n",
        "                predicted_noise = model(x, t, layout)\n",
        "                uncond_predicted_noise = model(x, t, None)\n",
        "                predicted_noise = torch.lerp(uncond_predicted_noise, predicted_noise, 3)\n",
        "\n",
        "                alpha = self.alpha[t][:, None, None, None]\n",
        "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
        "                beta = self.beta[t][:, None, None, None]\n",
        "                if i > 1:\n",
        "                    noise = torch.randn_like(x)\n",
        "                else:\n",
        "                    noise = torch.zeros_like(x)\n",
        "                x = 1 / torch.sqrt(alpha) * (\n",
        "                            x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(\n",
        "                    beta) * noise\n",
        "\n",
        "        model.train()\n",
        "        x = (x.clamp(-1, 1) + 1) / 2\n",
        "        x = (x * 255).type(torch.uint8)\n",
        "        print(\"end sample\")\n",
        "        return x"
      ],
      "metadata": {
        "id": "t1l-cWptaX-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modules"
      ],
      "metadata": {
        "id": "y-b4grVJakDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EMA:\n",
        "    def __init__(self, beta):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "        self.step = 0\n",
        "\n",
        "    def update_model_average(self, ma_model, current_model):\n",
        "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
        "            old_weight, up_weight = ma_params.data, current_params.data\n",
        "            ma_params.data = self.update_average(old_weight, up_weight)\n",
        "\n",
        "    def update_average(self, old, new):\n",
        "        if old is None:\n",
        "            return new\n",
        "        return old * self.beta + (1 - self.beta) * new\n",
        "\n",
        "    def step_ema(self, ema_model, model, step_start_ema=2000):\n",
        "        if self.step < step_start_ema:\n",
        "            self.reset_parameters(ema_model, model)\n",
        "            self.step += 1\n",
        "            return\n",
        "        self.update_model_average(ema_model, model)\n",
        "        self.step += 1\n",
        "\n",
        "    def reset_parameters(self, ema_model, model):\n",
        "        ema_model.load_state_dict(model.state_dict())\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, channels, size):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.size = size\n",
        "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
        "        self.ln = nn.LayerNorm([channels])\n",
        "        self.ff_self = nn.Sequential(\n",
        "            nn.LayerNorm([channels]),\n",
        "            nn.Linear(channels, channels),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(channels, channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n",
        "        x_ln = self.ln(x)\n",
        "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
        "        attention_value = attention_value + x\n",
        "        attention_value = self.ff_self(attention_value) + attention_value\n",
        "        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
        "        super().__init__()\n",
        "        self.residual = residual\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.GroupNorm(1, mid_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.GroupNorm(1, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.residual:\n",
        "            return F.gelu(x + self.double_conv(x))\n",
        "        else:\n",
        "            return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, emb_dim=128):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, in_channels, residual=True),\n",
        "            DoubleConv(in_channels, out_channels),\n",
        "        )\n",
        "\n",
        "        self.emd_layer = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(emb_dim, out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        x = self.maxpool_conv(x)\n",
        "        emb = self.emd_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
        "        return x + emb\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, emb_dim=128):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
        "        self.conv = nn.Sequential(\n",
        "            DoubleConv(in_channels, in_channels, residual=True),\n",
        "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
        "        )\n",
        "\n",
        "        self.emb_layer = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(emb_dim, out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip_x, t):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([skip_x, x], dim=1)\n",
        "        x = self.conv(x)\n",
        "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
        "        return x + emb\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, c_in=3, c_out=3, time_dim=256, device=\"cuda\"):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.time_dim = time_dim\n",
        "        self.inc = DoubleConv(c_in, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.sa1 = SelfAttention(128, 32)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.sa2 = SelfAttention(256, 16)\n",
        "        self.down3 = Down(256, 256)\n",
        "        self.sa3 = SelfAttention(256, 8)\n",
        "\n",
        "        self.bot1 = DoubleConv(256, 512)\n",
        "        self.bot2 = DoubleConv(512, 512)\n",
        "        self.bot3 = DoubleConv(512, 256)\n",
        "\n",
        "        self.up1 = Up(512, 128)\n",
        "        self.sa4 = SelfAttention(128, 16)\n",
        "        self.up2 = Up(256, 64)\n",
        "        self.sa5 = SelfAttention(64, 32)\n",
        "        self.up3 = Up(128, 64)\n",
        "        self.sa6 = SelfAttention(64, 64)\n",
        "        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n",
        "\n",
        "    def sinusoidal_embedding(self, t, channels):\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2, device=self.device).float() / channels))\n",
        "        emb_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
        "        emb_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
        "        emb = torch.cat([emb_a, emb_b], dim=-1)\n",
        "        return emb\n",
        "\n",
        "    def forward(self, x, t, obj_vecs):\n",
        "        t = t.unsqueeze(-1).type(torch.float)\n",
        "        t = self.sinusoidal_embedding(t, self.time_dim)\n",
        "        if obj_vecs is not None:\n",
        "            t += obj_vecs\n",
        "\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1, t)\n",
        "        x3 = self.down2(x2, t)\n",
        "        x4 = self.down3(x3, t)\n",
        "\n",
        "        x4 = self.bot1(x4)\n",
        "        x4 = self.bot2(x4)\n",
        "        x4 = self.bot3(x4)\n",
        "\n",
        "        x = self.up1(x4, x3, t)\n",
        "        x = self.up2(x, x2, t)\n",
        "        x = self.up3(x, x1, t)\n",
        "        output = self.outc(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "p8GqCApZamfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCN"
      ],
      "metadata": {
        "id": "B8rJflWNdqHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _init_weights(module):\n",
        "    if hasattr(module, 'weight'):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.kaiming_normal_(module.weight)\n",
        "\n",
        "\n",
        "class GraphConv(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, pooling='avg', mlp_normalization='none'):\n",
        "        super(GraphConv, self).__init__()\n",
        "        if output_dim is None:\n",
        "            output_dim = input_dim\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.pooling = pooling\n",
        "\n",
        "        self.net1 = nn.Sequential(\n",
        "            nn.Linear(3 * input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 2 * hidden_dim + output_dim),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.net2 = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.ReLU())\n",
        "\n",
        "    def forward(self, obj_vecs, pred_vecs, edges):\n",
        "\n",
        "        dtype, device = obj_vecs.dtype, obj_vecs.device\n",
        "        O, T = obj_vecs.size(0), pred_vecs.size(0)\n",
        "        Din, H, Dout = self.input_dim, self.hidden_dim, self.output_dim\n",
        "\n",
        "        s_idx = edges[:, 0].contiguous()\n",
        "        o_idx = edges[:, 1].contiguous()\n",
        "\n",
        "        cur_s_vecs = obj_vecs[s_idx]\n",
        "        cur_o_vecs = obj_vecs[o_idx]\n",
        "\n",
        "        cur_t_vecs = torch.cat([cur_s_vecs, pred_vecs, cur_o_vecs], dim=1)\n",
        "        new_t_vecs = self.net1(cur_t_vecs)\n",
        "\n",
        "        new_s_vecs = new_t_vecs[:, :H]\n",
        "        new_p_vecs = new_t_vecs[:, H:(H + Dout)]\n",
        "        new_o_vecs = new_t_vecs[:, (H + Dout):(2 * H + Dout)]\n",
        "\n",
        "        pooled_obj_vecs = torch.zeros(O, H, dtype=dtype, device=device)\n",
        "\n",
        "        s_idx_exp = s_idx.view(-1, 1).expand_as(new_s_vecs)\n",
        "        o_idx_exp = o_idx.view(-1, 1).expand_as(new_o_vecs)\n",
        "        pooled_obj_vecs = pooled_obj_vecs.scatter_add(0, s_idx_exp, new_s_vecs)\n",
        "        pooled_obj_vecs = pooled_obj_vecs.scatter_add(0, o_idx_exp, new_o_vecs)\n",
        "\n",
        "        if self.pooling == 'avg':\n",
        "            obj_counts = torch.zeros(O, dtype=dtype, device=device)\n",
        "            ones = torch.ones(T, dtype=dtype, device=device)\n",
        "            obj_counts = obj_counts.scatter_add(0, s_idx, ones)\n",
        "            obj_counts = obj_counts.scatter_add(0, o_idx, ones)\n",
        "\n",
        "            obj_counts = obj_counts.clamp(min=1)\n",
        "            pooled_obj_vecs = pooled_obj_vecs / obj_counts.view(-1, 1)\n",
        "\n",
        "        new_obj_vecs = self.net2(pooled_obj_vecs)\n",
        "\n",
        "        return new_obj_vecs, new_p_vecs\n",
        "\n",
        "\n",
        "class GraphConvNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=5, pooling='avg'):\n",
        "        super(GraphConvNet, self).__init__()\n",
        "\n",
        "        self.obj_embedding = nn.Embedding(185, 128)\n",
        "        self.pred_embedding = nn.Embedding(11, 128)\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.gcn = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.gcn.append(GraphConv(input_dim, hidden_dim, output_dim, pooling=pooling))\n",
        "\n",
        "    def forward(self, objs, preds, edges):\n",
        "        obj_vecs = self.obj_embedding(objs)\n",
        "        pred_vecs = self.pred_embedding(preds)\n",
        "        for i in range(self.num_layers):\n",
        "            layer = self.gcn[i]\n",
        "            obj_vecs, pred_vecs = layer(obj_vecs, pred_vecs, edges)\n",
        "        return obj_vecs"
      ],
      "metadata": {
        "id": "giPGHn_7dr-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "M8ta03hIbIhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_images(images, path):\n",
        "    grid = tutils.make_grid(images)\n",
        "    ndarr = grid.permute(1, 2, 0).to('cpu').numpy()\n",
        "    imgs = Image.fromarray(ndarr)\n",
        "    imgs.save(path)\n",
        "    return imgs\n",
        "\n",
        "def load_transfer_weights_biases(transfer_model_path, device):\n",
        "    checkpoint = torch.load(transfer_model_path, device)\n",
        "    original_od = checkpoint['model_state']\n",
        "    transfer_weights_biases = OrderedDict()\n",
        "\n",
        "    # object embedding\n",
        "    transfer_weights_biases['obj_embedding.weight'] = original_od['obj_embeddings.weight']\n",
        "\n",
        "    # pred embedding\n",
        "    transfer_weights_biases['pred_embedding.weight'] = original_od['pred_embeddings.weight']\n",
        "\n",
        "    # Layer_0\n",
        "    transfer_weights_biases['gcn.0.net1.0.weight'] = original_od['gconv.net1.0.weight']\n",
        "    transfer_weights_biases['gcn.0.net1.0.bias'] = original_od['gconv.net1.0.bias']\n",
        "    transfer_weights_biases['gcn.0.net1.2.weight'] = original_od['gconv.net1.2.weight']\n",
        "    transfer_weights_biases['gcn.0.net1.2.bias'] = original_od['gconv.net1.2.bias']\n",
        "    transfer_weights_biases['gcn.0.net2.0.weight'] = original_od['gconv.net2.0.weight']\n",
        "    transfer_weights_biases['gcn.0.net2.0.bias'] = original_od['gconv.net2.0.bias']\n",
        "    transfer_weights_biases['gcn.0.net2.2.weight'] = original_od['gconv.net2.2.weight']\n",
        "    transfer_weights_biases['gcn.0.net2.2.bias'] = original_od['gconv.net2.2.bias']\n",
        "\n",
        "    # Layer_1 - Layer_4\n",
        "    for i in range(1, 5):\n",
        "        transfer_weights_biases[f'gcn.{i}.net1.0.weight'] = original_od[f'gconv_net.gconvs.{i - 1}.net1.0.weight']\n",
        "        transfer_weights_biases[f'gcn.{i}.net1.0.bias'] = original_od[f'gconv_net.gconvs.{i - 1}.net1.0.bias']\n",
        "        transfer_weights_biases[f'gcn.{i}.net1.2.weight'] = original_od[f'gconv_net.gconvs.{i - 1}.net1.2.weight']\n",
        "        transfer_weights_biases[f'gcn.{i}.net1.2.bias'] = original_od[f'gconv_net.gconvs.{i - 1}.net1.2.bias']\n",
        "        transfer_weights_biases[f'gcn.{i}.net2.0.weight'] = original_od[f'gconv_net.gconvs.{i - 1}.net2.0.weight']\n",
        "        transfer_weights_biases[f'gcn.{i}.net2.0.bias'] = original_od[f'gconv_net.gconvs.{i - 1}.net2.0.bias']\n",
        "        transfer_weights_biases[f'gcn.{i}.net2.2.weight'] = original_od[f'gconv_net.gconvs.{i - 1}.net2.2.weight']\n",
        "        transfer_weights_biases[f'gcn.{i}.net2.2.bias'] = original_od[f'gconv_net.gconvs.{i - 1}.net2.2.bias']\n",
        "\n",
        "    return transfer_weights_biases\n",
        "\n",
        "\n",
        "\n",
        "def get_graph_emb(gcn, objs, triples):\n",
        "    O, T = objs.size(0), triples.size(0)\n",
        "    s, p, o = triples.chunk(3, dim=1)\n",
        "    s, p, o = [x.squeeze(1) for x in [s, p, o]]\n",
        "    preds = p\n",
        "    edges = torch.stack([s, o], dim=1)\n",
        "    with torch.no_grad():\n",
        "        obj_vecs = gcn(objs, preds, edges)\n",
        "    graph_emb = torch.sum(obj_vecs, dim=0, keepdim=True)\n",
        "    return graph_emb\n",
        "\n",
        "\n",
        "\n",
        "def process_graph(graph_path, dict_path):\n",
        "    with open(graph_path, \"r\") as inputfile:\n",
        "        graph = json.load(inputfile)\n",
        "        \n",
        "    with open(dict_path, \"r\") as inputfile:\n",
        "        graph_dict = json.load(inputfile)\n",
        "        \n",
        "    objs = graph[\"objs\"]\n",
        "    triples = graph[\"triples\"]\n",
        "    \n",
        "    edges = graph_dict[\"edges\"]\n",
        "    categories = graph_dict[\"categories\"]\n",
        "    \n",
        "    for obj in objs:\n",
        "        triples.append([obj, \"__in_image__\", \"in_image\"])\n",
        "    objs.append(\"in_image\")\n",
        "    \n",
        "    for i, triple in enumerate(triples):\n",
        "        for o, obj in enumerate(objs):\n",
        "            if obj == triple[0]:\n",
        "                triples[i][0] = o\n",
        "            if obj == triple[2]:\n",
        "                triples[i][2] = o\n",
        "        triple[1] = edges[triple[1]]\n",
        "    \n",
        "    for i, obj in enumerate(objs):\n",
        "        objs[i] = categories[obj]\n",
        "        \n",
        "    objs = torch.LongTensor(objs)\n",
        "    triples = torch.LongTensor(triples)\n",
        "    \n",
        "    return objs, triples"
      ],
      "metadata": {
        "id": "jnmHSB2KbKVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample"
      ],
      "metadata": {
        "id": "990VFxbWdz0B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUCoeG9dZWll"
      },
      "outputs": [],
      "source": [
        "def sample_images(sample_size=1, image_path=\"samled_images.jpg\", scene_graph=True, graph_path=\"sample_graph.json\"):\n",
        "    device = 'cuda'\n",
        "    embedding_dim = 128\n",
        "    dict_path = \"graph_dict.json\"\n",
        "    transfer_module_path = \"trained_models/transfer_models/coco64.pt\"\n",
        "    model_path = \"trained_models/scenegen_epoch_359/model_epoch_359.pt\"\n",
        "    ema_model_path = \"trained_models/scenegen_epoch_359/ema_model_epoch_359.pt\"\n",
        "    \n",
        "\n",
        "    sample_layouts = None\n",
        "    if scene_graph == True:\n",
        "        gcn = GraphConvNet(input_dim=embedding_dim, hidden_dim=512, output_dim=embedding_dim)\n",
        "        transfer_weights_biases = load_transfer_weights_biases(transfer_module_path, device)\n",
        "        gcn.load_state_dict(transfer_weights_biases)\n",
        "\n",
        "        objs, triples = process_graph(graph_path, dict_path)\n",
        "        graph_emb = get_graph_emb(gcn, objs, triples)\n",
        "\n",
        "        sample_layouts = []\n",
        "        for _ in range(sample_size):\n",
        "            sample_layouts.append(graph_emb)\n",
        "\n",
        "        sample_layouts = torch.cat(sample_layouts)\n",
        "        sample_layouts = sample_layouts.to(device)\n",
        "\n",
        "    model = UNet(time_dim=embedding_dim, device=device).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "\n",
        "    ema = EMA(0.995)\n",
        "    ema_model = copy.deepcopy(model).eval().requires_grad_(False)\n",
        "    ema_model.load_state_dict(torch.load(ema_model_path))\n",
        "    diffusion = Diffusion(device=device)\n",
        "    ema_sampled_images = diffusion.sample(ema_model, n=sample_size, layout=sample_layouts)\n",
        "    imgs = save_images(ema_sampled_images, image_path)\n",
        "    return imgs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "Qc7vm9kIPXqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = sample_images(sample_size=8, image_path=\"samled_images.jpg\", graph_path=\"sample_graph.json\")\n",
        "display(images)"
      ],
      "metadata": {
        "id": "66Oy26LdA15E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}